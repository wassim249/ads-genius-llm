{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning and Evaluation of Phi-2 Model for Ad Generation\n",
    "\n",
    "This notebook demonstrates the process of fine-tuning the Phi-2 model from Hugging Face for generating advertisements. It includes the following steps:\n",
    "\n",
    "1. **Environment Setup**:\n",
    "    - Install required libraries and dependencies.\n",
    "\n",
    "2. **Dataset Preparation**:\n",
    "    - Load and preprocess the dataset for training.\n",
    "\n",
    "3. **Tokenizer Setup**:\n",
    "    - Configure the tokenizer for the Phi-2 model.\n",
    "\n",
    "4. **Model Configuration**:\n",
    "    - Load the Phi-2 model with 4-bit quantization for efficient training.\n",
    "    - Apply LoRA (Low-Rank Adaptation) for parameter-efficient fine-tuning.\n",
    "\n",
    "5. **Training**:\n",
    "    - Define training arguments and train the model using the Hugging Face `Trainer`.\n",
    "\n",
    "6. **Text Generation**:\n",
    "    - Generate advertisements based on sample prompts using beam search and other decoding strategies.\n",
    "\n",
    "7. **Evaluation**:\n",
    "    - Evaluate the generated text using BLEU and ROUGE metrics.\n",
    "\n",
    "8. **Model Saving and Deployment**:\n",
    "    - Save the fine-tuned model and tokenizer locally and push them to the Hugging Face Hub.\n",
    "\n",
    "This notebook is designed to help you understand the end-to-end process of fine-tuning a language model for a specific task, in this case, generating creative advertisements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "# Environment Setup\n",
    "\n",
    "In this section, we will install the necessary libraries and dependencies required for fine-tuning the Phi-2 model. This includes installing the Hugging Face Transformers library, datasets, and other essential tools for model quantization, evaluation, and training.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-09T15:49:01.820081Z",
     "iopub.status.busy": "2025-03-09T15:49:01.819749Z",
     "iopub.status.idle": "2025-03-09T15:49:05.215691Z",
     "shell.execute_reply": "2025-03-09T15:49:05.214799Z",
     "shell.execute_reply.started": "2025-03-09T15:49:01.820051Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\21270\\anaconda3\\lib\\site-packages (4.32.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\21270\\anaconda3\\lib\\site-packages (2.12.0)\n",
      "Requirement already satisfied: peft in c:\\users\\21270\\anaconda3\\lib\\site-packages (0.15.0)\n",
      "Requirement already satisfied: torch in c:\\users\\21270\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: accelerate in c:\\users\\21270\\anaconda3\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: bitsandbytes in c:\\users\\21270\\anaconda3\\lib\\site-packages (0.45.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\21270\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\21270\\anaconda3\\lib\\site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\21270\\anaconda3\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\21270\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\21270\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\21270\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\21270\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\21270\\anaconda3\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\21270\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\21270\\anaconda3\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\21270\\anaconda3\\lib\\site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\21270\\anaconda3\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\21270\\anaconda3\\lib\\site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\21270\\anaconda3\\lib\\site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\21270\\anaconda3\\lib\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\21270\\anaconda3\\lib\\site-packages (from datasets) (2025.3.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\21270\\anaconda3\\lib\\site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\21270\\anaconda3\\lib\\site-packages (from datasets) (0.13.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\21270\\appdata\\roaming\\python\\python311\\site-packages (from peft) (5.9.4)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\21270\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\21270\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\21270\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\21270\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\21270\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\21270\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\21270\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\21270\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\21270\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\21270\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\21270\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\21270\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\21270\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\21270\\anaconda3\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: six in c:\\users\\21270\\appdata\\roaming\\python\\python311\\site-packages (from responses<0.19->datasets) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\21270\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\21270\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\21270\\appdata\\roaming\\python\\python311\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\21270\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\21270\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets peft torch accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T15:49:11.836131Z",
     "iopub.status.busy": "2025-03-09T15:49:11.835832Z",
     "iopub.status.idle": "2025-03-09T15:49:15.215958Z",
     "shell.execute_reply": "2025-03-09T15:49:15.214880Z",
     "shell.execute_reply.started": "2025-03-09T15:49:11.836106Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
      "Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.2.4)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.3.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.29.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.12)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate rouge_score nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "# Dataset Preparation\n",
    "\n",
    "In this section, we will load and preprocess the dataset required for fine-tuning the Phi-2 model. The dataset will be converted into a format compatible with the Hugging Face `datasets` library. This includes loading the dataset from a JSON file, converting it into a Hugging Face `Dataset` object, and displaying a sample entry for verification.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T15:49:30.822253Z",
     "iopub.status.busy": "2025-03-09T15:49:30.821907Z",
     "iopub.status.idle": "2025-03-09T15:49:31.761084Z",
     "shell.execute_reply": "2025-03-09T15:49:31.760332Z",
     "shell.execute_reply.started": "2025-03-09T15:49:30.822221Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Create an ad for my new organic juice brand, FreshPress, that emphasizes health benefits and taste, targeting health-conscious consumers.', 'ad_text': '🍏 **Quench Your Thirst, Boost Your Health!**\\\\n\\\\nIntroducing **FreshPress**: The Organic Juice that Delivers Taste & Nutrients!\\\\n\\\\n✨ *\"Tastes amazing and I feel fantastic!\"* - Jamie, Health Enthusiast\\\\n\\\\n👉 Join the **20,000+ Happy Customers** who’ve transformed their health!\\\\n\\\\n✅ **Organic Ingredients**: No additives, just real fruit!\\\\n✅ **Packed with Nutrition**: Each bottle delivers vitamins & minerals that support your immune system.\\\\n✅ **Guilt-Free Indulgence**: Enjoy refreshing flavors without the sugar crash!\\\\n\\\\n**Hurry, Limited Time Offer!**\\\\n🌟 Get **20% OFF** your first order! 🌟\\\\n\\\\n🛡️ **Risk-Free**: Enjoy our **30-Day Money-Back Guarantee!**\\\\n\\\\n**Ready to Revitalize Your Health?**\\\\n👉 *Click to Order Now!*\\\\n[Order Your FreshPress Juice Today] \\\\n\\\\n✨ *\"Best juice ever, I’m hooked!\"* - Alex, Repeat Customer\\\\n\\\\n#FreshPress #OrganicJuice #HealthGoals'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load your dataset\n",
    "with open('/kaggle/input/ads-list/fixed_ads_list.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_list(data)\n",
    "\n",
    "# Display a sample\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "# Tokenizer Setup\n",
    "\n",
    "In this section, we will configure the tokenizer for the Phi-2 model. The tokenizer is responsible for converting text into token IDs that the model can process. We will load the Phi-2 tokenizer, set the padding token to match the end-of-sequence token, and define a function to tokenize the dataset. The tokenized dataset will be used for training and evaluation.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T15:50:16.750687Z",
     "iopub.status.busy": "2025-03-09T15:50:16.750294Z",
     "iopub.status.idle": "2025-03-09T15:50:20.505145Z",
     "shell.execute_reply": "2025-03-09T15:50:20.504440Z",
     "shell.execute_reply.started": "2025-03-09T15:50:16.750661Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d890a564c2e74672824187b71c3978c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/485 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load Phi-2 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\")\n",
    "\n",
    "# Set the padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token  \n",
    "\n",
    "def tokenize_function(examples):\n",
    "    combined_texts = [f\"### Prompt: {p} ### Response: {a}\" for p, a in zip(examples[\"prompt\"], examples[\"ad_text\"])]\n",
    "    return tokenizer(combined_texts, truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "# Model Configuration\n",
    "\n",
    "In this section, we will load the Phi-2 model with 4-bit quantization for efficient training. The model will be configured using the `BitsAndBytesConfig` class from the `transformers` library. Additionally, we will apply LoRA (Low-Rank Adaptation) to enable parameter-efficient fine-tuning. This setup ensures that the model is optimized for both performance and resource efficiency.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T15:50:44.894572Z",
     "iopub.status.busy": "2025-03-09T15:50:44.894101Z",
     "iopub.status.idle": "2025-03-09T15:51:18.107794Z",
     "shell.execute_reply": "2025-03-09T15:51:18.106880Z",
     "shell.execute_reply.started": "2025-03-09T15:50:44.894529Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efabfd8a647b42fea9a2addecd84d22e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/35.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c29700a74c4457871d04199549b016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76bd19db4d9e4567889a232544a79c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "218a1c3528b54ffc92a5f920b41db6ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8c0fdd5fba14b7196b4596041a432cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c33d28271e4e678749549c616659c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Load the Phi-2 model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'microsoft/phi-2',\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T15:51:24.984816Z",
     "iopub.status.busy": "2025-03-09T15:51:24.984107Z",
     "iopub.status.idle": "2025-03-09T15:51:24.990936Z",
     "shell.execute_reply": "2025-03-09T15:51:24.990046Z",
     "shell.execute_reply.started": "2025-03-09T15:51:24.984781Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhiForCausalLM(\n",
      "  (model): PhiModel(\n",
      "    (embed_tokens): Embedding(51200, 2560)\n",
      "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x PhiDecoderLayer(\n",
      "        (self_attn): PhiSdpaAttention(\n",
      "          (q_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
      "          (k_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
      "          (v_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
      "          (dense): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
      "          (rotary_emb): PhiRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): PhiMLP(\n",
      "          (activation_fn): NewGELUActivation()\n",
      "          (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n",
      "          (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n",
      "        )\n",
      "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "    (rotary_emb): PhiRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Print model structure to find correct module names\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "# LoRA Configuration\n",
    "\n",
    "In this section, we will configure and apply Low-Rank Adaptation (LoRA) to the Phi-2 model. LoRA is a parameter-efficient fine-tuning technique that reduces the number of trainable parameters by introducing low-rank matrices. This allows for efficient adaptation of large language models to specific tasks while minimizing computational and memory overhead.\n",
    "\n",
    "We will define the LoRA configuration, specifying the target modules to adapt, and integrate it with the Phi-2 model. This setup ensures that the model is optimized for fine-tuning on the advertisement generation task.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T15:51:37.106861Z",
     "iopub.status.busy": "2025-03-09T15:51:37.106547Z",
     "iopub.status.idle": "2025-03-09T15:51:37.947465Z",
     "shell.execute_reply": "2025-03-09T15:51:37.946682Z",
     "shell.execute_reply.started": "2025-03-09T15:51:37.106838Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): PhiForCausalLM(\n",
      "      (model): PhiModel(\n",
      "        (embed_tokens): Embedding(51200, 2560)\n",
      "        (embed_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x PhiDecoderLayer(\n",
      "            (self_attn): PhiSdpaAttention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=2560, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=2560, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=2560, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (dense): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=2560, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (rotary_emb): PhiRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): PhiMLP(\n",
      "              (activation_fn): NewGELUActivation()\n",
      "              (fc1): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2560, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=10240, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (fc2): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=10240, out_features=2560, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=10240, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=2560, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
      "        (rotary_emb): PhiRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# ✅ LoRA Configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=32,  \n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"dense\", \"fc1\", \"fc2\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "# Training Configuration and Execution\n",
    "\n",
    "In this section, we will define the training configuration and execute the fine-tuning process for the Phi-2 model. This includes setting up training arguments, specifying evaluation and logging strategies, and configuring the data collator for causal language modeling. \n",
    "\n",
    "We will use the Hugging Face `Trainer` class to manage the training loop, which simplifies the process of fine-tuning by handling tasks such as gradient accumulation, checkpointing, and evaluation. Additionally, we will implement early stopping to terminate training if the model's performance does not improve after a specified number of evaluations.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T15:52:00.420974Z",
     "iopub.status.busy": "2025-03-09T15:52:00.420645Z",
     "iopub.status.idle": "2025-03-09T20:04:14.508389Z",
     "shell.execute_reply": "2025-03-09T20:04:14.507641Z",
     "shell.execute_reply.started": "2025-03-09T15:52:00.420946Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [180/180 4:11:24, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>11.359600</td>\n",
       "      <td>1.308570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>9.931100</td>\n",
       "      <td>1.166003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9.512300</td>\n",
       "      <td>1.098728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>8.968400</td>\n",
       "      <td>1.055438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>8.790000</td>\n",
       "      <td>1.026137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=180, training_loss=9.789615037706163, metrics={'train_runtime': 15133.4838, 'train_samples_per_second': 0.192, 'train_steps_per_second': 0.012, 'total_flos': 2.3395211280384e+16, 'train_loss': 9.789615037706163, 'epoch': 5.823045267489712})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=1e-4,  \n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,  \n",
    "    num_train_epochs=6,\n",
    "    weight_decay=0.01,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    report_to=\"none\",\n",
    "    push_to_hub=False,\n",
    "    load_best_model_at_end=True,  \n",
    "    metric_for_best_model=\"loss\",  \n",
    "    greater_is_better=False,  \n",
    ")\n",
    "\n",
    "# Data collator for language modeling (for causal LM)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,  \n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]  # Stops if no improvement after 2 evals\n",
    ")\n",
    "\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "# Text Generation\n",
    "\n",
    "In this section, we will generate advertisements based on sample prompts using the fine-tuned Phi-2 model. The text generation process involves providing a prompt to the model and decoding the output to produce coherent and creative advertisements. \n",
    "\n",
    "We will explore different decoding strategies such as greedy decoding, beam search, and temperature sampling to optimize the quality and diversity of the generated text. The generated advertisements will be evaluated for relevance, creativity, and adherence to the given prompts.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T20:04:14.515113Z",
     "iopub.status.busy": "2025-03-09T20:04:14.514876Z",
     "iopub.status.idle": "2025-03-09T20:04:24.239571Z",
     "shell.execute_reply": "2025-03-09T20:04:24.238520Z",
     "shell.execute_reply.started": "2025-03-09T20:04:14.515094Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introducing our latest product: \n",
      "\n",
      "**SmartHome Hub** - Control Your Home with Ease!\n",
      "\n",
      "**\"I can't believe how easy it is to manage my home!\" - Lisa R.**\n",
      "\n",
      "🌟 **Join 10,000+ Happy Homeowners!**\n",
      "🌟 **95% of users report increased convenience!**\n",
      "\n",
      "**What Makes SmartHome Hub Special?**\n",
      "\n",
      "• **Voice Control:** Simply say what you want and let\n"
     ]
    }
   ],
   "source": [
    "# Generate text based on a sample prompt\n",
    "sample_prompt = \"Introducing our latest product: \"\n",
    "inputs = tokenizer(sample_prompt, return_tensors='pt').to('cuda')\n",
    "output = model.generate(**inputs, max_length=100)\n",
    "\n",
    "# Decode and print the generated text\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T20:04:24.240811Z",
     "iopub.status.busy": "2025-03-09T20:04:24.240600Z",
     "iopub.status.idle": "2025-03-09T20:05:18.562139Z",
     "shell.execute_reply": "2025-03-09T20:05:18.561284Z",
     "shell.execute_reply.started": "2025-03-09T20:04:24.240792Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 **Prompt:** Create an ad for my new organic juice brand, FreshPress, that emphasizes health benefits and taste, targeting health-conscious consumers.\n",
      "🔹 **Generated Ad:** Create an ad for my new organic juice brand, FreshPress, that emphasizes health benefits and taste, targeting health-conscious consumers.\n",
      "## INPUT\n",
      "FreshPress Organic Juice Brand\n",
      "##OUTPUT\n",
      "**Quench Your Thirst with Freshness!**\\n\\nIntroducing **FreshPress Organic Juice Brand**: Where Health Meets Taste!\\n\\n\"I can’t get enough of these juices! They’re so delicious and good for you!\" - Sarah L., Health Enthusiast\\n\\n🍊 **Join 10,000+ Health-Conscious Drinkers!**\\n🍹 **4.8⭐ Rating on Trustpilot!**\\n\\n**Why Choose FreshPress?**\\n- **100% Organic Ingredients:** No preservatives, just pure goodness!\\n- **Vibrant Flavors:** From refreshing citrus to tropical delights!\\n- **Nutrient-Dense:** Packed with vitamins and minerals!\\n\\n**Limited Time Offer:**\\n✓ **Free Shipping on Orders Over $20!**\\n✓ **30-Day Money-Back Guarantee!**\\n\n"
     ]
    }
   ],
   "source": [
    "sample_prompt = \"Create an ad for my new organic juice brand, FreshPress, that emphasizes health benefits and taste, targeting health-conscious consumers.\"\n",
    "\n",
    "# Generate text using beam search\n",
    "inputs = tokenizer(sample_prompt, return_tensors='pt').to('cuda')\n",
    "output = model.generate(\n",
    "    **inputs, \n",
    "    max_length=256,  \n",
    "    num_beams=5,  # Beam search for better quality\n",
    "    temperature=0.7,  # Adds diversity to outputs\n",
    "    repetition_penalty=1.2  # Reduces word repetition\n",
    ")\n",
    "\n",
    "# Decode and print results\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"🔹 **Prompt:**\", sample_prompt)\n",
    "print(\"🔹 **Generated Ad:**\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T20:05:18.563456Z",
     "iopub.status.busy": "2025-03-09T20:05:18.563011Z",
     "iopub.status.idle": "2025-03-09T20:06:10.887796Z",
     "shell.execute_reply": "2025-03-09T20:06:10.886887Z",
     "shell.execute_reply.started": "2025-03-09T20:05:18.563286Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 **Prompt:** Create an ad for my new organic juice brand, FreshPress, that emphasizes health benefits and taste, targeting health-conscious consumers.\n",
      "🔹 **Generated Ad:** 🍊 **Quench Your Thirst with FreshPress!**\\n\\nIntroducing **FreshPress**: The Organic Juice You Can Trust!\\n\\n\"*I never knew juice could taste this good and be so good for me!*\" - Jenna L., Health Enthusiast\\n\\n🌟 **Join 20,000+ Health-Conscious Drinkers!**\\n🌟 **4.8⭐ Rating on Trustpilot!**\\n\\n**Why Choose FreshPress?**\\n\\n• **100% Organic Ingredients:** No preservatives, just pure goodness!\\n• **Vibrant Flavors:** From refreshing citrus to antioxidant-packed berries!\\n• **Nutrient-Rich:** Boost your immune system with every sip!\\n\\n**Limited Time Offer:**\\n✓ **Buy 3 Bottles, Get 1 Free!**\\n✓ **30-Day Money-Back Guarantee!**\\n\\n**Don’t Miss Out\n"
     ]
    }
   ],
   "source": [
    "sample_prompt = \"Create an ad for my new organic juice brand, FreshPress, that emphasizes health benefits and taste, targeting health-conscious consumers.\"\n",
    "\n",
    "# Format it the same way as during training\n",
    "formatted_prompt = f\"### Prompt: {sample_prompt} ### Response:\"\n",
    "\n",
    "# Tokenize and generate\n",
    "inputs = tokenizer(formatted_prompt, return_tensors='pt').to('cuda')\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_length=256,\n",
    "    num_beams=5,\n",
    "    temperature=0.7,\n",
    "    repetition_penalty=1.2,\n",
    "    pad_token_id=tokenizer.eos_token_id  # important if padding token is needed\n",
    ")\n",
    "\n",
    "# Decode and clean\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Strip the prefix to get just the response\n",
    "if \"### Response:\" in generated_text:\n",
    "    generated_ad = generated_text.split(\"### Response:\")[1].strip()\n",
    "else:\n",
    "    generated_ad = generated_text.strip()\n",
    "\n",
    "print(\"🔹 **Prompt:**\", sample_prompt)\n",
    "print(\"🔹 **Generated Ad:**\", generated_ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T20:32:26.949104Z",
     "iopub.status.busy": "2025-03-09T20:32:26.948654Z",
     "iopub.status.idle": "2025-03-09T20:33:19.978428Z",
     "shell.execute_reply": "2025-03-09T20:33:19.977631Z",
     "shell.execute_reply.started": "2025-03-09T20:32:26.949065Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 **Prompt:** Create an ad for my new organic juice brand, FreshPress, that emphasizes health benefits and taste, targeting health-conscious consumers.\n",
      "🔹 **Generated Ad:**\n",
      "🍊 **Quench Your Thirst with FreshPress!**\n",
      "Introducing **FreshPress**: The Organic Juice You Can Trust!\n",
      "\"*I never knew juice could taste this good and be so good for me!*\" - Jenna L., Health Enthusiast\n",
      "🌟 **Join 20,000+ Health-Conscious Drinkers!**\n",
      "🌟 **4.8⭐ Rating on Trustpilot!**\n",
      "**Why Choose FreshPress?**\n",
      "• **100% Organic Ingredients:** No preservatives, just pure goodness!\n",
      "• **Vibrant Flavors:** From refreshing citrus to antioxidant-packed berries!\n",
      "• **Nutrient-Rich:** Boost your immune system with every sip!\n",
      "**Limited Time Offer:**\n",
      "✓ **Buy 3 Bottles, Get 1 Free!**\n",
      "✓ **30-Day Money-Back Guarantee!**\n",
      "**Don’t Miss Out\n"
     ]
    }
   ],
   "source": [
    "sample_prompt = \"Create an ad for my new organic juice brand, FreshPress, that emphasizes health benefits and taste, targeting health-conscious consumers.\"\n",
    "\n",
    "# Match the training format\n",
    "formatted_prompt = f\"### Prompt: {sample_prompt} ### Response:\"\n",
    "\n",
    "# Tokenize and move to device\n",
    "inputs = tokenizer(formatted_prompt, return_tensors='pt').to('cuda')\n",
    "\n",
    "# Generate text\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_length=256,\n",
    "    num_beams=5,\n",
    "    temperature=0.7,\n",
    "    repetition_penalty=1.2,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode output\n",
    "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# 🔍 Extract only the response part\n",
    "if \"### Response:\" in decoded_output:\n",
    "    generated_ad = decoded_output.split(\"### Response:\")[1]\n",
    "else:\n",
    "    generated_ad = decoded_output\n",
    "\n",
    "def clean_output(text):\n",
    "    # Remove leading prompt echoes or structured markers\n",
    "    for marker in [\"### Prompt:\", \"##INPUT\", \"##OUTPUT\"]:\n",
    "        if marker in text:\n",
    "            text = text.split(marker)[-1]\n",
    "    # Convert literal \\n to actual newlines\n",
    "    text = text.replace(\"\\\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\")\n",
    "    # Strip leading/trailing whitespace\n",
    "    return text.strip()\n",
    "\n",
    "# Final clean ad\n",
    "final_ad = clean_output(generated_ad)\n",
    "\n",
    "print(\"🔹 **Prompt:**\", sample_prompt)\n",
    "print(\"🔹 **Generated Ad:**\")\n",
    "print(final_ad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "# Evaluation of Generated Text\n",
    "\n",
    "In this section, we will evaluate the quality of the generated advertisements using BLEU and ROUGE metrics. These metrics provide a quantitative measure of the similarity between the generated text and the reference text, helping us assess the relevance, fluency, and overall quality of the model's outputs.\n",
    "\n",
    "- **BLEU (Bilingual Evaluation Understudy)**: Measures the overlap of n-grams between the generated text and the reference text.\n",
    "- **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**: Focuses on recall and measures the overlap of n-grams, word sequences, and word pairs.\n",
    "\n",
    "The evaluation results will help us understand the performance of the fine-tuned Phi-2 model on the advertisement generation task.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T20:22:54.558241Z",
     "iopub.status.busy": "2025-03-09T20:22:54.557913Z",
     "iopub.status.idle": "2025-03-09T20:22:56.522830Z",
     "shell.execute_reply": "2025-03-09T20:22:56.522092Z",
     "shell.execute_reply.started": "2025-03-09T20:22:54.558218Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e7906ce2174482a16b1d2be1d258d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fbd84e6ed1b40d2b933f1b6551271b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59074e9b1f37450cad8a865dfc00ad84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fceb3ee0e994b9f9808ed7beb73e73a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 BLEU Score: {'bleu': 0.22388790591605398, 'precisions': [0.6563876651982379, 0.3805309734513274, 0.18666666666666668, 0.10714285714285714], 'brevity_penalty': 0.8421423919980932, 'length_ratio': 0.8533834586466166, 'translation_length': 227, 'reference_length': 266}\n",
      "🔹 ROUGE Score: {'rouge1': 0.49808429118773945, 'rouge2': 0.23166023166023164, 'rougeL': 0.3524904214559387, 'rougeLsum': 0.3524904214559387}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "# Load BLEU and ROUGE metrics\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "# Sample generated vs. ground truth\n",
    "reference = [dataset[0][\"ad_text\"]]\n",
    "candidate = generated_text\n",
    "\n",
    "# Compute BLEU\n",
    "bleu_score = bleu.compute(predictions=[candidate], references=[[reference]])\n",
    "rouge_score = rouge.compute(predictions=[candidate], references=[reference])\n",
    "\n",
    "print(\"🔹 BLEU Score:\", bleu_score)\n",
    "print(\"🔹 ROUGE Score:\", rouge_score)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6740622,
     "sourceId": 10852713,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
