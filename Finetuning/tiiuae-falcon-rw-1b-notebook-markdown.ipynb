{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e4af2f7",
   "metadata": {},
   "source": [
    "# Falcon-RW-1B Fine-Tuning Notebook\n",
    "This notebook demonstrates how to fine-tune the Falcon-RW-1B model using LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-20T00:19:36.677386Z",
     "iopub.status.busy": "2025-03-20T00:19:36.677010Z",
     "iopub.status.idle": "2025-03-20T00:19:43.538594Z",
     "shell.execute_reply": "2025-03-20T00:19:43.537743Z",
     "shell.execute_reply.started": "2025-03-20T00:19:36.677348Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.45.3\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets peft torch accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0433bb19",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies\n",
    "We first install the necessary Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T00:19:43.540303Z",
     "iopub.status.busy": "2025-03-20T00:19:43.539983Z",
     "iopub.status.idle": "2025-03-20T00:19:49.081540Z",
     "shell.execute_reply": "2025-03-20T00:19:49.080469Z",
     "shell.execute_reply.started": "2025-03-20T00:19:43.540271Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.2.4)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.3.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.29.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.17.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.12)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2025.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=3b6a0aec6a44fc69eb2525798eb149503c0aadd9379135d8f136699f449473b9\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: rouge_score, evaluate\n",
      "Successfully installed evaluate-0.4.3 rouge_score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate rouge_score nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T00:19:50.879782Z",
     "iopub.status.busy": "2025-03-20T00:19:50.879440Z",
     "iopub.status.idle": "2025-03-20T00:19:54.309157Z",
     "shell.execute_reply": "2025-03-20T00:19:54.308257Z",
     "shell.execute_reply.started": "2025-03-20T00:19:50.879751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.45.3)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7adbd7",
   "metadata": {},
   "source": [
    "## 2. Load and Preprocess Dataset\n",
    "We load a JSON dataset and convert it into a Hugging Face dataset format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T00:19:49.083673Z",
     "iopub.status.busy": "2025-03-20T00:19:49.083345Z",
     "iopub.status.idle": "2025-03-20T00:19:50.877872Z",
     "shell.execute_reply": "2025-03-20T00:19:50.877134Z",
     "shell.execute_reply.started": "2025-03-20T00:19:49.083639Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Create an ad for my new organic juice brand, FreshPress, that emphasizes health benefits and taste, targeting health-conscious consumers.', 'ad_text': 'üçè **Quench Your Thirst, Boost Your Health!**\\\\n\\\\nIntroducing **FreshPress**: The Organic Juice that Delivers Taste & Nutrients!\\\\n\\\\n‚ú® *\"Tastes amazing and I feel fantastic!\"* - Jamie, Health Enthusiast\\\\n\\\\nüëâ Join the **20,000+ Happy Customers** who‚Äôve transformed their health!\\\\n\\\\n‚úÖ **Organic Ingredients**: No additives, just real fruit!\\\\n‚úÖ **Packed with Nutrition**: Each bottle delivers vitamins & minerals that support your immune system.\\\\n‚úÖ **Guilt-Free Indulgence**: Enjoy refreshing flavors without the sugar crash!\\\\n\\\\n**Hurry, Limited Time Offer!**\\\\nüåü Get **20% OFF** your first order! üåü\\\\n\\\\nüõ°Ô∏è **Risk-Free**: Enjoy our **30-Day Money-Back Guarantee!**\\\\n\\\\n**Ready to Revitalize Your Health?**\\\\nüëâ *Click to Order Now!*\\\\n[Order Your FreshPress Juice Today] \\\\n\\\\n‚ú® *\"Best juice ever, I‚Äôm hooked!\"* - Alex, Repeat Customer\\\\n\\\\n#FreshPress #OrganicJuice #HealthGoals'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load your dataset\n",
    "with open('/kaggle/input/ads-list/fixed_ads_list.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_list(data)\n",
    "\n",
    "# Display a sample\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3806946",
   "metadata": {},
   "source": [
    "## 3. Tokenization\n",
    "We load the Falcon-RW-1B tokenizer and prepare our dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T00:19:54.310447Z",
     "iopub.status.busy": "2025-03-20T00:19:54.310180Z",
     "iopub.status.idle": "2025-03-20T00:20:03.830106Z",
     "shell.execute_reply": "2025-03-20T00:20:03.829127Z",
     "shell.execute_reply.started": "2025-03-20T00:19:54.310423Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a5e2fdf1934e79a635e3843f699260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "289f24fde8854743b04098a29e75f6e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a4751b4b9bb4deeaf47133a33b63568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc60ddd8ccc4f469817ea324bbfbc6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb68426271d6488d9ae1723acffe50cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2cc556e23494d638484ccab7b07878d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/485 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load Falcon-RW-1B tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-rw-1b\")\n",
    "\n",
    "# Check if the model has a pad_token, if not use the eos_token as fallback\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  \n",
    "\n",
    "def tokenize_function(examples):\n",
    "    combined_texts = [f\"### Prompt: {p} ### Response: {a}\" for p, a in zip(examples[\"prompt\"], examples[\"ad_text\"])]\n",
    "    model_inputs = tokenizer(combined_texts, truncation=True, padding=\"max_length\", max_length=512)\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"]  # Add labels as input_ids for causal LM\n",
    "    return model_inputs\n",
    "\n",
    "# Apply tokenization to the dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccf45d0",
   "metadata": {},
   "source": [
    "## 4. Load Pretrained Model with Quantization\n",
    "We use `BitsAndBytesConfig` to load the Falcon model with 4-bit quantization to optimize memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T00:20:03.831980Z",
     "iopub.status.busy": "2025-03-20T00:20:03.831622Z",
     "iopub.status.idle": "2025-03-20T00:20:37.124147Z",
     "shell.execute_reply": "2025-03-20T00:20:37.123127Z",
     "shell.execute_reply.started": "2025-03-20T00:20:03.831945Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b09676858341fca038fe4b9567b563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c20d813d7574143b0c855c992bbd0e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_falcon.py:   0%|          | 0.00/6.70k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-rw-1b:\n",
      "- configuration_falcon.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a617d9ce95ae406c8c1e11d029e8bbbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_falcon.py:   0%|          | 0.00/56.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-rw-1b:\n",
      "- modeling_falcon.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca4ce0a6fa4437aaecedce6ee3d852c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.62G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "103c9cd9e78a4d1baa2462914fd1d8b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/115 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Load the Falcon-RW-1B model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'tiiuae/falcon-rw-1b',\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T00:20:37.125887Z",
     "iopub.status.busy": "2025-03-20T00:20:37.125175Z",
     "iopub.status.idle": "2025-03-20T00:20:37.133487Z",
     "shell.execute_reply": "2025-03-20T00:20:37.132579Z",
     "shell.execute_reply.started": "2025-03-20T00:20:37.125847Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FalconForCausalLM(\n",
      "  (transformer): FalconModel(\n",
      "    (word_embeddings): Embedding(50304, 2048)\n",
      "    (h): ModuleList(\n",
      "      (0-23): 24 x FalconDecoderLayer(\n",
      "        (self_attention): FalconAttention(\n",
      "          (query_key_value): Linear4bit(in_features=2048, out_features=6144, bias=True)\n",
      "          (dense): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp): FalconMLP(\n",
      "          (dense_h_to_4h): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (dense_4h_to_h): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
      "        )\n",
      "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=50304, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Print model structure to find correct module names\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bda57a",
   "metadata": {},
   "source": [
    "## 5. Configure LoRA for Parameter Efficient Fine-Tuning\n",
    "LoRA reduces the number of trainable parameters while maintaining performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T00:20:37.137214Z",
     "iopub.status.busy": "2025-03-20T00:20:37.136921Z",
     "iopub.status.idle": "2025-03-20T00:20:46.868033Z",
     "shell.execute_reply": "2025-03-20T00:20:46.867006Z",
     "shell.execute_reply.started": "2025-03-20T00:20:37.137193Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): FalconForCausalLM(\n",
      "      (transformer): FalconModel(\n",
      "        (word_embeddings): Embedding(50304, 2048)\n",
      "        (h): ModuleList(\n",
      "          (0-23): 24 x FalconDecoderLayer(\n",
      "            (self_attention): FalconAttention(\n",
      "              (query_key_value): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=6144, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=6144, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (dense): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (mlp): FalconMLP(\n",
      "              (dense_h_to_4h): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=8192, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act): GELU(approximate='none')\n",
      "              (dense_4h_to_h): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=8192, out_features=32, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=32, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "            )\n",
      "            (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "            (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (lm_head): lora.Linear(\n",
      "        (base_layer): Linear(in_features=2048, out_features=50304, bias=False)\n",
      "        (lora_dropout): ModuleDict(\n",
      "          (default): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (lora_A): ModuleDict(\n",
      "          (default): Linear(in_features=2048, out_features=32, bias=False)\n",
      "        )\n",
      "        (lora_B): ModuleDict(\n",
      "          (default): Linear(in_features=32, out_features=50304, bias=False)\n",
      "        )\n",
      "        (lora_embedding_A): ParameterDict()\n",
      "        (lora_embedding_B): ParameterDict()\n",
      "        (lora_magnitude_vector): ModuleDict()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py:543: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# ‚úÖ LoRA Configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=32,  \n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"query_key_value\", \"dense\", \"dense_h_to_4h\", \"dense_4h_to_h\", \"lm_head\"]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ec88a0",
   "metadata": {},
   "source": [
    "## 6. Training Setup and Execution\n",
    "We define training arguments and train the model using the Hugging Face `Trainer` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T00:20:46.869745Z",
     "iopub.status.busy": "2025-03-20T00:20:46.869409Z",
     "iopub.status.idle": "2025-03-20T01:15:41.602126Z",
     "shell.execute_reply": "2025-03-20T01:15:41.601404Z",
     "shell.execute_reply.started": "2025-03-20T00:20:46.869721Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [180/180 54:40, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.481700</td>\n",
       "      <td>1.391018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.262800</td>\n",
       "      <td>1.214674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.182700</td>\n",
       "      <td>1.123979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.096600</td>\n",
       "      <td>1.062722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.036600</td>\n",
       "      <td>1.013454</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:230: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=180, training_loss=1.2240387068854437, metrics={'train_runtime': 3293.1567, 'train_samples_per_second': 0.884, 'train_steps_per_second': 0.055, 'total_flos': 1.07216747102208e+16, 'train_loss': 1.2240387068854437, 'epoch': 5.823045267489712})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=1e-4,  \n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,  \n",
    "    num_train_epochs=6,\n",
    "    weight_decay=0.01,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    report_to=\"none\",\n",
    "    push_to_hub=False,\n",
    "    load_best_model_at_end=True,  \n",
    "    metric_for_best_model=\"loss\",  \n",
    "    greater_is_better=False,  \n",
    ")\n",
    "\n",
    "# Data collator for language modeling (for causal LM)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,  \n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]  # Stops if no improvement after 2 evals\n",
    ")\n",
    "\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d548406",
   "metadata": {},
   "source": [
    "## 7. Text Generation\n",
    "Finally, we generate text based on a sample input prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T01:15:41.603084Z",
     "iopub.status.busy": "2025-03-20T01:15:41.602880Z",
     "iopub.status.idle": "2025-03-20T01:15:46.025508Z",
     "shell.execute_reply": "2025-03-20T01:15:46.024790Z",
     "shell.execute_reply.started": "2025-03-20T01:15:41.603065Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introducing our latest product: \n",
      "üöÄ **Introducing the Ultimate Fitness Tracker!**\n",
      "‚úÖ **Track Your Progress with Precision!**\n",
      "‚úÖ **Customizable Workouts!**\n",
      "‚úÖ **Connected to Your Smartphone!**\n",
      "‚ö° **Join 50,000+ Happy Users!**\n",
      "‚ö° **95% of Users Report Increased Fitness!**\n",
      "‚ö° **Trusted by Top Fitness Influencers!**\n",
      "üëâ **[\n"
     ]
    }
   ],
   "source": [
    "# Generate text based on a sample prompt\n",
    "sample_prompt = \"Introducing our latest product: \"\n",
    "inputs = tokenizer(sample_prompt, return_tensors='pt').to('cuda')\n",
    "output = model.generate(**inputs, max_length=100)\n",
    "\n",
    "# Decode and print the generated text\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T01:15:46.026512Z",
     "iopub.status.busy": "2025-03-20T01:15:46.026262Z",
     "iopub.status.idle": "2025-03-20T01:16:04.734624Z",
     "shell.execute_reply": "2025-03-20T01:16:04.733917Z",
     "shell.execute_reply.started": "2025-03-20T01:15:46.026491Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ **Prompt:** Create an ad for my new organic juice brand, FreshPress, that emphasizes health benefits and taste, targeting health-conscious consumers.\n",
      "üîπ **Generated Ad:** Create an ad for my new organic juice brand, FreshPress, that emphasizes health benefits and taste, targeting health-conscious consumers.\n",
      "‚Äî FreshPress Juice (@FreshPressJuice)\n",
      "üçä **Join the FreshPress Movement!** üçä\n",
      "‚Üí 95% of our customers say they feel better after drinking our juices!\n",
      "‚Äî FreshPress Juice (@FreshPressJuice)\n",
      "üî• **Limited Time Offer: Get 20% off your first order!** üî•\n",
      "‚Äî FreshPress Juice (@FreshPressJuice)\n",
      "üëâ [Order Now] üëà\n",
      "‚Äî FreshPress Juice (@FreshPressJuice)\n",
      "üåü \"Best organic juice I‚Äôve ever tasted!\" - Sarah, Health Enthusiast üåü\n",
      "‚Äî FreshPress Juice (@FreshPressJuice)\n",
      "üëâ **Join the FreshPress Movement!** üëà\n",
      "‚Äî FreshPress Juice (@FreshPressJuice)\n",
      "Join the FreshPress Revolution! ‚¨ÖÔ∏è\n",
      "‚Äî FreshPress Juice (@FreshPressJuice)\n",
      "üëâ [Try FreshPress] üëà\n",
      "‚Äî FreshPress Juice (@FreshPressJuice)\n",
      "üî• **Join the FreshPress Community!** üî•\n",
      "‚Äî FreshPress Juice (@\n"
     ]
    }
   ],
   "source": [
    "sample_prompt = \"Create an ad for my new organic juice brand, FreshPress, that emphasizes health benefits and taste, targeting health-conscious consumers.\"\n",
    "\n",
    "# Generate text using beam search\n",
    "inputs = tokenizer(sample_prompt, return_tensors='pt').to('cuda')\n",
    "output = model.generate(\n",
    "    **inputs, \n",
    "    max_length=256,  \n",
    "    num_beams=5,  # Beam search for better quality\n",
    "    temperature=0.7,  # Adds diversity to outputs\n",
    "    repetition_penalty=1.2  # Reduces word repetition\n",
    ")\n",
    "\n",
    "# Decode and print results\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"üîπ **Prompt:**\", sample_prompt)\n",
    "print(\"üîπ **Generated Ad:**\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prompt = \"Create an ad for my new organic juice brand, FreshPress, that emphasizes health benefits and taste, targeting health-conscious consumers.\"\n",
    "\n",
    "# Ensure the model understands it's a completion task\n",
    "formatted_prompt = f\"### Prompt: {sample_prompt} ### Response:\"\n",
    "\n",
    "inputs = tokenizer(formatted_prompt, return_tensors='pt').to('cuda')\n",
    "output = model.generate(\n",
    "    **inputs, \n",
    "    max_length=512,  # Increase max length\n",
    "    num_beams=3,  # Reduce beams for more diverse output\n",
    "    temperature=0.9,  # Increase temperature for variation\n",
    "    repetition_penalty=1.1,  # Reduce penalty for smoother text\n",
    "    top_p=0.9,  # Use nucleus sampling to increase diversity\n",
    "    do_sample=True,  # Enable sampling to reduce repetition\n",
    "    pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "# Decode and print results\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"üîπ **Prompt:**\", sample_prompt)\n",
    "print(\"üîπ **Generated Ad:**\", generated_text.replace(formatted_prompt, \"\").strip())  # Remove repeated prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T01:16:04.735817Z",
     "iopub.status.busy": "2025-03-20T01:16:04.735469Z",
     "iopub.status.idle": "2025-03-20T01:16:23.442475Z",
     "shell.execute_reply": "2025-03-20T01:16:23.441737Z",
     "shell.execute_reply.started": "2025-03-20T01:16:04.735781Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introducing our latest product: \n",
      "Introducing our latest product: \n",
      "üöÄ **Join 50,000+ Satisfied Users!**\n",
      "üöÄ **95% of Users Recommend Us!**\n",
      "üöÄ **4.8‚≠ê Rating on Trustpilot!**\n",
      "üöÄ **Exclusive Launch Offer: Buy One, Get One 50% OFF!**\n",
      "üëâ **What‚Äôs Inside?**\n",
      "üåü **Instant Access to All Features:**\n",
      "‚úÖ **Track Your Progress:** See how far you‚Äôve come!\n",
      "‚úÖ **Customized Plans:** Tailored to your needs!\n",
      "‚úÖ **User-Friendly Interface:** Easy to use for everyone!\n",
      "üëâ **Limited Time Offer:**\n",
      "‚úÖ **30-Day Money-Back Guarantee!**\n",
      "‚úÖ **Risk-Free 30-Day Trial!**\n",
      "üëâ **Don‚Äôt Miss Out!**\n",
      "‚ö° **Hurry! Limited Time Offer!** ‚ö°\n",
      "üëâ **[Buy Now]**\n",
      "üëâ **Transform Your Health Today!**\n",
      "#\n"
     ]
    }
   ],
   "source": [
    "# Generate text based on a sample prompt\n",
    "sample_prompt = \"Introducing our latest product: \"\n",
    "inputs = tokenizer(sample_prompt, return_tensors='pt').to('cuda')\n",
    "output = model.generate(\n",
    "    **inputs, \n",
    "    max_length=256,  \n",
    "    num_beams=5,  # Beam search for better quality\n",
    "    temperature=0.7,  # Adds diversity to outputs\n",
    "    repetition_penalty=1.2  # Reduces word repetition\n",
    ")\n",
    "\n",
    "# Decode and print the generated text\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T01:16:23.443441Z",
     "iopub.status.busy": "2025-03-20T01:16:23.443205Z",
     "iopub.status.idle": "2025-03-20T01:16:26.644655Z",
     "shell.execute_reply": "2025-03-20T01:16:26.643834Z",
     "shell.execute_reply.started": "2025-03-20T01:16:23.443412Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccd103348f7344a9a23de6b9fe89882e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df2bd987c60a41358c40b90f0c63fb55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d30eeb7ec2944cc19e0bad6b067907ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24dbfd18c5d2498a9facc52d7222d5a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ BLEU Score: {'bleu': 0.03673709674942967, 'precisions': [0.358695652173913, 0.14207650273224043, 0.038461538461538464, 0.0055248618784530384], 'brevity_penalty': 0.6404064834081792, 'length_ratio': 0.6917293233082706, 'translation_length': 184, 'reference_length': 266}\n",
      "üîπ ROUGE Score: {'rouge1': 0.3231441048034934, 'rouge2': 0.10572687224669604, 'rougeL': 0.21834061135371177, 'rougeLsum': 0.22707423580786026}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "# Load BLEU and ROUGE metrics\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "# Sample generated vs. ground truth\n",
    "reference = [dataset[0][\"ad_text\"]]\n",
    "candidate = generated_text\n",
    "\n",
    "# Compute BLEU\n",
    "bleu_score = bleu.compute(predictions=[candidate], references=[[reference]])\n",
    "rouge_score = rouge.compute(predictions=[candidate], references=[reference])\n",
    "\n",
    "print(\"üîπ BLEU Score:\", bleu_score)\n",
    "print(\"üîπ ROUGE Score:\", rouge_score)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6740622,
     "sourceId": 10852713,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
